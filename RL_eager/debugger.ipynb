{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "\n",
    "def preprocess(observation):\n",
    "    # RGB to Gray-Scale, to size 80*80\n",
    "    observation = cv2.cvtColor(cv2.resize(observation, (80, 105)), cv2.COLOR_BGR2GRAY)[25:,:]\n",
    "    return np.reshape(observation, (80, 80, 1)).astype(\"float32\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class ReplayMemory:\n",
    "    def __init__(self, max_length):\n",
    "        self.memory = deque(maxlen=max_length)\n",
    "\n",
    "    def add(self, state, action, reward, next_state, terminal):\n",
    "        self.memory.append([state, action, reward, next_state, terminal])\n",
    "\n",
    "    def get_batch(self, batch_size):\n",
    "        sampling = np.array(random.sample(self.memory, batch_size))\n",
    "        state_batch = np.stack(sampling[:, 0])\n",
    "        next_state_batch = np.stack(sampling[:, 3])\n",
    "        return state_batch, sampling[:, 1], sampling[:, 2], next_state_batch, sampling[:, 4]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow.contrib.eager as tfe\n",
    "\n",
    "# eager execution\n",
    "tfe.enable_eager_execution(device_policy=tfe.DEVICE_PLACEMENT_SILENT)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyper parameter\n",
    "INITIAL_EPSILON = 1.0  # initial exploration rate\n",
    "FINAL_EPSILON = 0.05  # final exploration rate\n",
    "LEARNING_RATE = 0.001  # learning rate\n",
    "OBSERVATION_STEPS = 50000  # step for observing(not trainig)\n",
    "EXPLORATION_STEPS = 500000  # step for exploration(epsilon > FINAL_EPSILON)\n",
    "BATCH_SIZE = 32  # batch size\n",
    "GAMMA = 0.97  # discount rate\n",
    "\n",
    "\n",
    "class DQNAgent(tf.keras.Model):\n",
    "    def __init__(self,\n",
    "                 state_shape=(-1,80,80,1),\n",
    "                 action_dim=4,\n",
    "                 checkpoint_directory=\"models_checkpoints/rl/\",\n",
    "                 batch_size=BATCH_SIZE,\n",
    "                 initial_epsilon= INITIAL_EPSILON,\n",
    "                 final_epsilon=FINAL_EPSILON,\n",
    "                 exploration_steps=EXPLORATION_STEPS,\n",
    "                 observation_steps=OBSERVATION_STEPS,\n",
    "                 device_name='cpu:0'):\n",
    "\n",
    "        super(DQNAgent, self).__init__()\n",
    "        # state's shape , in Atari we will use (-1, 105, 80, 1)\n",
    "        self.state_shape = state_shape\n",
    "        # number of actions, in Atari 4\n",
    "        self.action_dim = action_dim\n",
    "        # saving checkpoint directory\n",
    "        self.checkpoint_directory = checkpoint_directory\n",
    "\n",
    "        self.observation_steps=observation_steps\n",
    "        self.exploration_steps = exploration_steps\n",
    "        self.initial_epsilon=initial_epsilon\n",
    "        self.final_epsilon=final_epsilon\n",
    "\n",
    "        # init q layers\n",
    "        self.conv1 = tf.layers.Conv2D(32, 8, 8, padding='same', activation=tf.nn.relu)\n",
    "        self.batch1 = tf.layers.BatchNormalization()\n",
    "        self.conv2 = tf.layers.Conv2D(64, 4, 4, padding='same', activation=tf.nn.relu)\n",
    "        self.batch2 = tf.layers.BatchNormalization()\n",
    "        self.conv3 = tf.layers.Conv2D(64, 3, 3, padding='same', activation=tf.nn.relu)\n",
    "        self.flatten = tf.layers.Flatten()\n",
    "\n",
    "        self.dense1 = tf.layers.Dense(512, activation=tf.nn.relu)\n",
    "        self.dense2 = tf.layers.Dense(action_dim, activation=None)\n",
    "\n",
    "        self.base_layers = [self.conv1, self.batch1, self.conv2, self.batch2, self.conv3, self.flatten, self.dense1,\n",
    "                            self.dense2]\n",
    "\n",
    "        # target q layers\n",
    "        self.conv1_t = tf.layers.Conv2D(32, 8, 8, padding='same', activation=tf.nn.relu)\n",
    "        self.batch1_t = tf.layers.BatchNormalization()\n",
    "        self.conv2_t = tf.layers.Conv2D(64, 4, 4, padding='same', activation=tf.nn.relu)\n",
    "        self.batch2_t = tf.layers.BatchNormalization()\n",
    "        self.conv3_t = tf.layers.Conv2D(64, 3, 3, padding='same', activation=tf.nn.relu)\n",
    "        self.flatten_t = tf.layers.Flatten()\n",
    "\n",
    "        self.dense1_t = tf.layers.Dense(512, activation=tf.nn.relu)\n",
    "        self.dense2_t = tf.layers.Dense(action_dim, activation=None)\n",
    "\n",
    "        self.target_layers = [self.conv1_t, self.batch1_t, self.conv2_t, self.batch2_t, self.conv3_t, self.flatten_t,\n",
    "                              self.dense1_t, self.dense2_t]\n",
    "\n",
    "        # learning optimizer\n",
    "        self.optimizer = tf.train.AdamOptimizer(LEARNING_RATE)\n",
    "\n",
    "        # epsilon-greedy\n",
    "        self.epsilon = initial_epsilon\n",
    "        self.epsilon_step = (initial_epsilon - final_epsilon) / exploration_steps\n",
    "\n",
    "        # replay_memory\n",
    "        self.replay_memory = ReplayMemory(100000)\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "        # for logging\n",
    "        self.step_count = 0\n",
    "        self.sum_loss = 0;\n",
    "\n",
    "        # device configuration\n",
    "        self.device_name = device_name\n",
    "\n",
    "    def predict(self, state_batch, training):\n",
    "\n",
    "        # you can use prediction with numpy array state input\n",
    "        if isinstance(state_batch, (np.ndarray, np.generic)):\n",
    "            state_batch = np.reshape(state_batch, self.state_shape)\n",
    "            state_batch = tf.convert_to_tensor(state_batch)\n",
    "\n",
    "        x = self.conv1(state_batch)\n",
    "        x = self.batch1(x, training=training)\n",
    "        x = self.conv2(x)\n",
    "        x = self.batch2(x, training=training)\n",
    "        x = self.conv3(x)\n",
    "        x = self.flatten(x)\n",
    "        x = self.dense1(x)\n",
    "        x = self.dense2(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "    def predict_target(self, state_batch, training):\n",
    "\n",
    "        # you can use prediction with numpy array state input\n",
    "        if isinstance(state_batch, (np.ndarray, np.generic)):\n",
    "            state_batch = np.reshape(state_batch, self.state_shape)\n",
    "            state_batch = tf.convert_to_tensor(state_batch)\n",
    "\n",
    "        x = self.conv1_t(state_batch)\n",
    "        x = self.batch1_t(x, training=training)\n",
    "        x = self.conv2_t(x)\n",
    "        x = self.batch2_t(x, training=training)\n",
    "        x = self.conv3_t(x)\n",
    "        x = self.flatten_t(x)\n",
    "        x = self.dense1_t(x)\n",
    "        x = self.dense2_t(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "    def copy_base_to_target(self):\n",
    "        \"\"\"copy base's weights to target\"\"\"\n",
    "        for idx_layer in range(len(self.base_layers)):\n",
    "            base = self.base_layers[idx_layer]\n",
    "            target = self.target_layers[idx_layer]\n",
    "            for idx_weight in range(len(base.weights)):\n",
    "                tf.assign(target.weights[idx_weight], base.weights[idx_weight])\n",
    "            if hasattr(base, \"bias\"):\n",
    "                tf.assign(target.bias, base.bias)\n",
    "\n",
    "    @staticmethod\n",
    "    def huber_loss(labels, predictions):\n",
    "        error = labels - predictions\n",
    "        quadratic_term = error * error / 2\n",
    "        linear_term = abs(error) - 1 / 2     \n",
    "        use_linear_term = tf.convert_to_tensor((abs(error) > 1.0).numpy().astype(\"float32\"))\n",
    "        return use_linear_term * linear_term + (1 - use_linear_term) * quadratic_term\n",
    "\n",
    "    def loss(self, state_batch, target, training):\n",
    "        predictions = self.predict(state_batch, training)\n",
    "        # loss_value = tf.losses.mean_squared_error(labels=target, predictions=predictions)\n",
    "        loss_value = self.huber_loss(labels=target, predictions=predictions)\n",
    "        self.sum_loss += tf.reduce_sum(loss_value).numpy()\n",
    "        return loss_value\n",
    "\n",
    "    def grad(self, state_batch, target, training):\n",
    "        with tfe.GradientTape() as tape:\n",
    "            loss_value = self.loss(state_batch, target, training)\n",
    "        return tape.gradient(loss_value, self.variables)\n",
    "\n",
    "    def get_action(self, state, training=False):\n",
    "        if training:\n",
    "            if self.epsilon >= random.random():\n",
    "                action = tf.convert_to_tensor(random.randrange(self.action_dim))\n",
    "            else:\n",
    "                action = tf.argmax(self.predict(state, training=training), 1)\n",
    "\n",
    "            if self.epsilon > self.final_epsilon and self.step_count > self.observation_steps:\n",
    "                self.epsilon -= self.epsilon_step\n",
    "\n",
    "            return action\n",
    "\n",
    "        else:\n",
    "            return tf.argmax(self.predict(state, training=training), 1)\n",
    "\n",
    "    def step(self, state, action, reward, next_state, terminal):\n",
    "        if self.step_count <= self.observation_steps:\n",
    "            self.observe(state, action, reward, next_state, terminal)\n",
    "        else:\n",
    "            self.fit(state, action, reward, next_state, terminal)\n",
    "\n",
    "        if self.step_count % 1000 == 0:\n",
    "            print(\"STEP %s : EPSILON [%6f]....\" % (self.step_count, self.epsilon))\n",
    "            print(\"=============================================\")\n",
    "        self.step_count += 1\n",
    "\n",
    "    def observe(self, state, action, reward, next_state, terminal):\n",
    "        self.replay_memory.add(state, action, reward, next_state, terminal)\n",
    "\n",
    "    def fit(self, state, action, reward, next_state, terminal, num_epochs=1):\n",
    "\n",
    "        self.replay_memory.add(state, action, reward, next_state, terminal)\n",
    "\n",
    "        state_batch, action_batch, reward_batch, next_state_batch, terminal_batch = self.replay_memory.get_batch(\n",
    "            self.batch_size)\n",
    "\n",
    "        current_q = self.predict(state_batch, training=False).numpy()\n",
    "        now_q = np.zeros((self.batch_size,self.action_dim))\n",
    "\n",
    "        target_q_batch = self.predict_target(next_state_batch, training=False)\n",
    "\n",
    "        y_batch = reward_batch + (1 - terminal_batch) * GAMMA * np.max(target_q_batch, axis=1)\n",
    "\n",
    "        for i in range(self.batch_size):\n",
    "            now_q[i, action_batch[i]] = y_batch[i]\n",
    "\n",
    "        with tf.device(self.device_name):\n",
    "            for i in range(num_epochs):\n",
    "                grads = self.grad(state_batch, now_q, True)\n",
    "                self.optimizer.apply_gradients(zip(grads, self.variables))\n",
    "\n",
    "        if self.step_count % 1000 == 0 :\n",
    "            print(\"loss: %6f\" % (self.sum_loss/1000))\n",
    "            self.sum_loss = 0\n",
    "            print(current_q[0])\n",
    "            print(now_q[0])\n",
    "            print(self.predict(state_batch[0], training=False).numpy())\n",
    "\n",
    "        return\n",
    "\n",
    "    def save(self, global_step=0):\n",
    "        tfe.Saver(self.variables).save(self.checkpoint_directory, global_step=global_step)\n",
    "\n",
    "    def load_last_checkpoint(self):\n",
    "        # Run the model once to initialize variables\n",
    "        initialshape = list(self.state_shape)\n",
    "        initialshape[0] = 1\n",
    "        initialshape = tuple(initialshape)\n",
    "        dummy_input = tf.constant(tf.zeros(initialshape))\n",
    "        dummy_pred = self.predict(dummy_input, training=False)\n",
    "        # Restore the variables of the model\n",
    "        saver = tfe.Saver(self.variables)\n",
    "        saver.restore(tf.train.latest_checkpointflaot16\n",
    "                      (self.checkpoint_directory))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STEP 0 : EPSILON [1.000000]....\n",
      "=============================================\n",
      "\u001b[31m#############################################\n",
      "Episode 0 finished after 236 timesteps\n",
      "reward[-49 - 0]: 0.020000\n",
      "epsilon: 1.0\n",
      "236 step - 0.64947104454 sec\n",
      "average time for step : 0.00275199595144\n",
      "#############################################\n",
      "\u001b[0m\n",
      "STEP 1000 : EPSILON [1.000000]....\n",
      "=============================================\n"
     ]
    }
   ],
   "source": [
    "from colorama import Fore, Style\n",
    "import time\n",
    "import gym\n",
    "\n",
    "env = gym.make('Breakout-v0')\n",
    "agent = DQNAgent(state_shape=(-1, 80, 80, 1),\n",
    "                 action_dim=4,\n",
    "                 checkpoint_directory=\"./models_checkpoints/rl/\",\n",
    "                 batch_size=32,\n",
    "                 initial_epsilon=1.0,\n",
    "                 final_epsilon=0.05,\n",
    "                 exploration_steps=300000,\n",
    "                 observation_steps=1000,\n",
    "                 device_name=\"gpu:0\")\n",
    "\n",
    "# agent.load_last_checkpoint()\n",
    "total_reward = 0.0\n",
    "episode_step_50 = 0\n",
    "time_50=time.time()\n",
    "for i_episode in range(100000):\n",
    "    observation = env.reset()\n",
    "    for t in range(10000000):\n",
    "        # env.render()\n",
    "        now_state = preprocess(observation)\n",
    "        action = agent.get_action(now_state, training=True).numpy()\n",
    "        observation, reward, done, info = env.step(action)\n",
    "\n",
    "        if (done):\n",
    "            done = 1\n",
    "        else:\n",
    "            done = 0\n",
    "\n",
    "        next_state = preprocess(observation)\n",
    "        agent.step(now_state, action, reward, next_state, done)\n",
    "        total_reward += reward\n",
    "        if done:\n",
    "            if agent.step_count > agent.observation_steps:\n",
    "                agent.copy_base_to_target()\n",
    "\n",
    "            if i_episode % 50 == 0:\n",
    "                spend_time = time.time() - time_50\n",
    "                total_step = agent.step_count - episode_step_50\n",
    "\n",
    "                print(Fore.RED + \"#############################################\")\n",
    "                print(\"Episode {} finished after {} timesteps\".format(i_episode, t + 1))\n",
    "                print(\"reward[%d - %d]: %3f\" % (min([0,i_episode - 49]), i_episode, total_reward / 50))\n",
    "                print(\"epsilon: %s\" % agent.epsilon)\n",
    "                print(\"%s step - %s sec\" % (total_step, spend_time))\n",
    "                print(\"average time for step : %s\" % (spend_time/total_step))\n",
    "                print(\"#############################################\")\n",
    "                print(Style.RESET_ALL)\n",
    "                if agent.step_count > agent.observation_steps:\n",
    "                    agent.save(i_episode)\n",
    "\n",
    "                total_reward = 0.0\n",
    "                episode_step_50 = agent.step_count\n",
    "                time_50 = time.time()\n",
    "\n",
    "            break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "env = gym.make('Breakout-v0')\n",
    "agent = DQNAgent(state_shape=(-1, 80, 80, 1), \n",
    "                 action_dim=4,\n",
    "                 checkpoint_directory=\"./models_checkpoints/rl2/\", \n",
    "                 batch_size=1,\n",
    "                 observation_steps=500,\n",
    "                 device_name=\"gpu:0\")\n",
    "\n",
    "observation = env.reset()\n",
    "total_reward = 0\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "t1 = time.time()\n",
    "for i in range(500):\n",
    "    now_state= preprocess(observation)\n",
    "    action = agent.get_action(now_state, training=True).numpy()\n",
    "    observation, reward, done, info = env.step(action)\n",
    "    if(done):\n",
    "        done = 1\n",
    "        env.reset()\n",
    "    else:\n",
    "        done = 0\n",
    "    next_state= preprocess(observation)\n",
    "    agent.step(now_state, action, reward, next_state, done)\n",
    "    \n",
    "print(time.time()-t1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "now_state= preprocess(observation)\n",
    "action = agent.get_action(now_state, training=True).numpy()\n",
    "observation, reward, done, info = env.step(action)\n",
    "if(done):\n",
    "    done = 1\n",
    "    env.reset()\n",
    "else:\n",
    "    done = 0\n",
    "next_state= preprocess(observation)\n",
    "agent.step(now_state, action, reward, next_state, done)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "now_state= preprocess(observation)\n",
    "action = agent.get_action(now_state, training=True).numpy()\n",
    "observation, reward, done, info = env.step(action)\n",
    "if(done):\n",
    "    done = 1\n",
    "    env.reset()\n",
    "else:\n",
    "    done = 0\n",
    "next_state= preprocess(observation)\n",
    "agent.step(now_state, action, reward, next_state, done)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_batch, action_batch, reward_batch, next_state_batch, terminal_batch = agent.replay_memory.get_batch(\n",
    "            agent.batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_q = agent.predict(state_batch, training=False).numpy()\n",
    "float16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.replay_memory.memory[-1][4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.conv1.bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.conv1_t.bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.conv1.weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.conv1_t.weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.copy_base_to_target()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#             return action\n",
    "\n",
    "# for i_episode in range(10000):\n",
    "#     observation = env.reset()\n",
    "#     total_reward = 0\n",
    "#     for t in range(10000000):\n",
    "# #         env.render()\n",
    "#         now_state= preprocess(observation)\n",
    "#         action = agent.get_action(now_state, training=True).numpy()\n",
    "#         observation, reward, done, info = env.step(action)\n",
    "#         if(done):\n",
    "#             done = 1\n",
    "#         else:\n",
    "#             done = 0\n",
    "#         next_state= preprocess(observation)\n",
    "#         agent.step(now_state, action, reward, next_state, done)\n",
    "#         total_reward += reward\n",
    "#         if done:\n",
    "#             if agent.step_count > 5000:\n",
    "#                 agent.copy_base_to_target()\n",
    "\n",
    "#             if i_episode % 50 == 0:\n",
    "#                 print(\"Episode {} finished after {} timesteps\".format(i_episode,t+1))\n",
    "#                 print(\"reward: %d\" % total_reward)\n",
    "#                 print(\"epsilon: %s\"% agent.epsilon)\n",
    "#                 if agent.step_count > OBSERVATION_STEPS:\n",
    "#                     agent.save(i_episode)\n",
    "#             break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
